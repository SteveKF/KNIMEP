%% analyse.tex
\chapter{Konzept}
\label{ch:Konzept}
%% ==============================
Dieses Kapitel befasst sich darum, welche Algorithmen in KNIME implementiert wurden und erklärt deren Vorgehensweise. Zuerst wird der Block-Nested-Loop \cite{borzsony2001skyline}, ein Skylinealgorithmus gezeigt. Die Skyline wird von manchen repräsentativen Skylinealgorithmen zur Berechnung benötigt. Daher ist mindestens ein Skylinealgorithmus für die Arbeit zwingend notwendig. Zur Bestimmung von repräsentativen Skylines wird in dieser Arbeit als erstes der DominationMaximizer \cite{4221657} vorgestellt. Dieser versucht die $k$ Skylinedatensätze heraus zu suchen, die die Anzahl der dominierten Datensätze maximieren. Der zweite vorgestellte Algorithmus basiert auf Distanz \cite{Tao:2009:DRS:1546683.1547325} und sucht die Repräsentanten heraus, sodass die Distanz zwischen einem Nicht-Repräsentant und dem nächstliegendsten Repräsentant minimal ist. Der letzte vorgestellte Algorithmus ist der E-Greedy \cite{magnani2014taking}. Das Ziel dieses ist es die repräsentative Skyline durch Berücksichtigung von Diversität und Signifikanz der Datensätzen zu bestimmen.
Zum Schluss dieses Kapitels werden genauere Details zur Funktionsweise der drei wichtigen Klassen NodeModel, NodeDialog und NodeView von KNIME aufgeführt, die für die Implementierung der Algorithmen eine wichtige Rolle spielen.
%% ==============================
\section{Skyline-Algorithmen}
\label{ch:Analyse:sec:skyAlgos}
%% ==============================
Methoden zur Berechnung einer Skyline gibt es viele, wie zum Beispiel Nested-SQL Statements, Block-Nested-Loop, Divide \& Conquer, etc.
Zusätzlich gibt es auch viele Ansätze, die auf B/R-Bäume aufbauen, wie zum Beispiel der Algorithmus in \cite{Papadias:2003:OPA:872757.872814}. Der Vorteil dieser ist, dass sie während der Berechnung schon Datensätze der Skyline ausgeben können und im Fall von Suchabfragen dem User schon nach kurzer Zeit einige Ergebnisse liefern können. Weiterhin sind Algorithmen, die auf B/R-Bäumen basieren oft schneller als andere Algorithmen, da oft viele Vergleiche durch Pruning der Bäume oder mithilfe der Branch \& Bound Methode reduziert werden können. 
Da in dieser Arbeit der Mittelpunkt repräsentative Skyline sind, wurde für diese Arbeit nur der Block-Nested-Loop Algorithmus implementiert, da dieser intuitiv und für den Zweck dieser Arbeit ausreichend ist. Weitere Skyline-Algorithmen in KNIME einzubauen, wäre ein guter Schritt diese Arbeit fortzusetzen. 
%% ==============================
\subsection{Block-Nested-Loop}
\label{ch:Analyse:sec:skyAlgos:subsec:bnl}
%% ==============================
Der Block-Nested-Loop Algorithmus basiert auf einer einfachen geschachtelten Schleife. Zur Reduzierung der Rechenzeit wurde ein Fenster $w$ eingeführt, in welchem unvergleichbare bzw. undominierte Datensätze gespeichert werden. $w$ kann aber nur eine bestimmte Anzahl von Datensätze halten. Meistens bestimmt der User wie viele Datensätze im Fenster gespeichert werden können.
Beim Einlesen eines Datensatzes, wird er mit allen anderen Datensätzen in $w$ verglichen. Hier gibt es drei Möglichkeiten:
Falls der eingelesene Datensatz $p$ von einem oder mehreren Datensätzen in $w$ dominiert wird, wird er eliminiert und in folgenden Iterationen nicht mehr betrachtet. Dies führt zu großen Einsparungen der Rechenzeit. 
Falls $p$ jedoch einen oder mehrere Datensätze in $w$ dominiert, werden diese Datensätze von $w$ entfernt und $p$ wird zu $w$ hinzugefügt.
Bei Unvergleichbarkeit wird $p$ in $w$ eingefügt, falls in $w$ noch nicht die vom User bestimmte Grenze erreicht ist. Ansonsten wird $p$ in eine temporäre Datei geschrieben.

Die Datensätze in der temporären Datei werden in der nächsten Iteration mit dem Fenster verglichen. Dieser Vorgang wird so lange wiederholt bis nach einer Iteration kein Datensatz mehr in der temporären Datei vorhanden ist.
Damit dieser Vorgang terminiert, bekommen Datensätze beim Einfügen in $w$ oder in die temporäre Datei einen Zeitstempel. Beim Einlesen des Datensatzes, wird jeder Datensatz in $w$ mit einem geringeren Zeitstempel ausgegeben  bzw. in die Skyline eingefügt.
Weiterhin werden nach der ersten Iteration alle Datensätze aus $w$ zur Skyline hinzugefügt, bei der während des Hinzufügens in $w$ die temporäre Datei leer war.
Diese beiden Kriterien sorgen nicht nur für die sichere Terminierung des Algorithmus, sondern auch für eine Einsparung der Rechenzeit.

Ein Pseudo-Code für diesen Algorithmus befindet sich im Anhang von \cite{borzsony2001skyline}.

Da sich diese Arbeit hauptsächlich um repräsentative Skylines und Präferenzen befasst und trotzdem eine Implementierung eines Skylinealgorithmus erforderlich war, fiel die Entscheidung auf den Block-Nested-Loop. Dieser war einfach und schnell zu implementieren und liefert im Gegensatz zu einer normalen verschachtelten Schleife ein schnelleres Ergebnis.  
%% ==============================
\section{Repräsentative Skyline Algorithmen}
\label{ch:Analyse:sec:repSkyAlgos}
%% ==============================
In diesem Abschnitt des Kapitels werden die Algorithmen zur Berechnung einer repräsentativen Skyline, die für diese Arbeit in KNIME implementiert wurden, vorgestellt. 
Diese basieren hauptsächlich auf einen der zwei Ziele: Diversität oder Signifikanz zu maximieren (siehe Abschnitt \ref{ch:Grundlagen:sec:repSkyline}).
Zur Gegenüberstellung wurden sowohl Algorithmen implementiert, die den originalen Datensatz als Input nehmen als auch welche, die nur mit einer Skyline als Input arbeiten können.
%% ==============================
\subsection{DominationMaximizer}
\label{ch:Analyse:sec:repSkyAlgos:subsec:domMax}
%% ==============================
Der DominationMaximizer (Algorithmus \ref{algo:domMaximizer}) ist eine kleine Abwandlung des Greedy-Algorithmus, der in Kapitel 5.2 von \cite{4221657} vorgestellt wird. 
Das Ziel beider Algorithmen ist es mit $k$ Datensätzen die Anzahl an dominierten Datensätzen zu maximieren. 

\begin{algorithm}[H]
\caption{DominationMaximizer}\label{algo:domMaximizer}
\begin{algorithmic}[1]
\INPUTBF $k$ int, $P$ set of data points
\OUTPUTBF $k$ skyline points
\State $S_P = \varnothing$ and $S = \varnothing$
\State \textbf{for each} data point $p$ in $P$ 
\State \hspace{\algorithmicindent} \textbf{for each} data point $q$ in $P$
\State \hspace{\algorithmicindent} \hspace{\algorithmicindent} \textbf{if} $p \succ q$ \textbf{then} 
\State \hspace{\algorithmicindent} \hspace{\algorithmicindent} \hspace{\algorithmicindent} $S_P:= S_P \cup \{p\}$ and $S_P:= S_P-\{q\}$
\State \hspace{\algorithmicindent} \hspace{\algorithmicindent} \hspace{\algorithmicindent} $D(\{p\}):=D(\{p\}) \cup \{q\}$  
\State \hspace{\algorithmicindent} \hspace{\algorithmicindent} \textbf{else if} $q \succ p$ \textbf{then} 
\State \hspace{\algorithmicindent} \hspace{\algorithmicindent} \hspace{\algorithmicindent} $S_P:= S_P \cup \{q\}$ and $S_P:= S_P-\{p\}$  
\State \hspace{\algorithmicindent} \hspace{\algorithmicindent} \hspace{\algorithmicindent} $D(\{q\}):=D(\{q\}) \cup \{p\}$  
\State \textbf{while} $|S|<k$ and $S_P-S \neq \varnothing$ \textbf{do}
\State \hspace{\algorithmicindent} choose $s \in {S_P-S}$ such that $|D(\{s\} \cup S)|$ is maximized
\State \hspace{\algorithmicindent} $S:=\{s\} \cup S$
\State \textbf{return} $S$
\end{algorithmic}
\end{algorithm}

Im Gegensatz zum Greedy-Algorithmus berechnet der DominationMaximizer Algorithmus die Skyline und die Menge der dominierten Datensätze für jeden undominierten Datensatz gleichzeitig. Falls bei einem Vergleich zwischen zwei Datensätzen einer der beiden vom anderen dominiert wird, wird dieser Datensatz der Menge $D\{undominierter Datensatz\}$ hinzugefügt. $D\{s\}$ ist die Menge von Datensätzen, die von Datensatz $s$ dominiert werden. 

Nach dem Durchlaufen der Schleife werden $k$ Datensätze ausgewählt, die erstens von keinem anderen Datensatz dominiert werden und die die Menge an dominierten Datensätzen maximieren.

Der Greedy-Algorithmus legt nicht fest, welcher Algorithmus für die Berechnung der Skyline benötigt wird. Für den DominationMaximizer wird eine einfache verschachtelte Schleife verwendet, um sicher zu stellen, dass jeder Datensatz mit allen anderen verglichen wird. 

Das Problem dieses Algorithmus ist, dass bei angehäufte Daten nur Skylinedatensätze nahe angehäufter Bereiche in der repräsentativen Skyline enthalten sind, da diese die meisten Datensätze dominieren. Da Ausreißer meistens nur undominierte Datensätze entsprechen und keinen anderen Datensatz dominieren, werden diese meistens vom DominationMaximizer nicht zur Skyline hinzugefügt.  Dies hat zur Folge, dass die Repräsentationsgüte für solche Fälle relativ niedrig ist, was gut in Figur \ref{img:dominationMaximizerGraph} zu sehen ist.
 
\begin{figure}[H]
	\centering
	\includegraphics[width=1.1\textwidth]{dominationMaximizerGraph.png}
	\caption{Die repräsentative Skyline des DominationMaximizer bei angehäuften Daten}
	\label{img:dominationMaximizerGraph}
\end{figure}
%% ==============================
\subsection{Distanz-basierende repräsentative Skyline}
\label{ch:Analyse:sec:repSkyAlgos:subsec:disBasedRepSky}
%% ==============================
Angenommen die Menge $S$ beinhaltet alle Skylinedatensätze und $K$ entspricht einer repräsentativen Skyline mit $k$ Datensätzen. Mit diesen beiden Mengen kann nun der repräsentative Fehler $Er(K,S)$ berechnet werden. 
$$Er(K,S)=\max\limits_{p\in{S-K}}\{\min\limits_{p^{'} \in{K}}||p,p^{'}||\}$$
Entsprechend dieses Fehlers ist eine repräsentative Skyline gut, falls für jeden nicht-repräsentativen Skylinedatensatz $\in{S-K}$, ein repräsentativer Datensatz $\in{K}$ in der Nähe ist. An der Formel ist zu erkennen, dass der Fehler die Repräsentationsgüte anhand der Distanz zwischen einem nicht-repräsentativen Datensatz und dem nächstliegenden repräsentativen Datensatz misst. Diese Distanz wird mit der euklidischen Distanz berechnet.
Das Ziel dieses Algorithmus ist es die $k$ Datensätzen zu finden, mit denen dieser repräsentative Fehler minimal ist. Bedingung für die korrekte Durchführung des Algorithmus sind, dass alle Datensätze nur zwei Dimensionen besitzen oder nur zwei betrachtet werden. Zusätzlich sollten die Datensätze nach der ersten Dimension aufsteigend sortiert werden. Für dieses Arbeit wird der Algorithmus in KNIME implementiert und dadurch wird die Sortierung im entsprechenden KNIME Node durchgeführt, damit sich der User damit nicht befassen muss. 

Die Funktion $opt(i,t)$ bestimmt die optimale size-$t$ repräsentative Skyline von $S_i$, wobei $t \leq i$ gilt. $S_i$ ist hier eine Teilmenge von $S$ mit $i \leq m$ (m=Anzahl der Skylinedatensätze). Falls $i=m$ und somit $S_i=S$ gilt, entspricht $opt(m,k)$ der optimalen size-$k$ repräsentativen Skyline von $S$.
Um diese Funktion jedoch berechnen zu können, muss zuerst der optimale Fehler bestimmt werden.
$$optEr(i,t)=\min\limits_{j=t}^{i-1}\{max\{optEr(j-1,t-1),radius(j,i)\}\}$$
$i$ und $t$ sind hier zwei Indizes, die den Indizes von zwei Datensätzen der Skyline entsprechen. Die Funktion misst den Fehler anhand von Radien, die Kreise bilden. Diese Kreise umfassen wiederum Datensätzen der Skyline und einer der umfassenden Datensätze muss der Mittelpunkt des Kreises sein. Umso mehr Datensätze umfasst werden und umso geringer der Radius ist, desto kleiner ist der Fehler. 
Angenommen $v$ sei der Wert von $j$ bei dem oben genannte Funktion $optEr(i,t)$ ihr Minimum findet, dann ergibt sich die repräsentative Skyline durch folgende Funktion:
$$opt(i,t)=opt(v-1,t-1)\cup\{center(v,i)\}$$
Diese Funktion wird wie $optEr(i,t)$ rekursiv berechnet und fügt die Datensätze hinzu, die Mittelpunkte für die entsprechenden Indizes $v$ und $i$ sind. Dadurch das $v$ der Wert von $j$ ist, bei der die Funktion $optEr(i,t)$ ihr Minimum erreicht, wird erzwungen das nur Kreise betrachtet werden, die mit geringem Radius die meisten Datensätze umfassen.   
Für die Berechnung der Radien der umfassenden Kreise, wird die euklidische Distanz benutzt:
$$radius(i,j)=\min\limits_{u=i}^{j}\{max\{||p_i,p_u||,||p_u,p_j||\}\}$$
Der Punkt $p_u$ bei dem diese Funktion ihr Minimum erreicht, ist der Mittelpunkt des Kreises und somit ein Kandidat der repräsentativen Skyline.

\begin{algorithm}[H]
\caption{2D-opt ($S$,$k$)}\label{algo:2DOpt}
\begin{algorithmic}[1]
\INPUTBF the skyline $S$ of dataset $D$ and an integer $k$
\OUTPUTBF the representative skyline of $D$
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{\textbf{for each} pair of $(i,j)$ such that $1 \leq i \leq j \leq m$, derive\par 
$radius(i,j)$ and $center(i,j)$\strut}
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{set $opt(i,1) = \{center(1,i)\}$ and $optEr(i,1)=radius(1,i)$\par
\textbf{for each} $1 \leq i \leq m$\strut}
\State \textbf{for} $t=2$ \textbf{to} $k-1$
\State \hspace{\algorithmicindent} \textbf{for} $i=t$ \textbf{to} $m$
\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} compute $optEr(i,t)$
\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} compute $opt(i,t)$
\State compute $optEr(m,k)$ and $opt(m,k)$
\State \textbf{return} $opt(m,k)$
\end{algorithmic}
\end{algorithm}

Algorithmus \ref{algo:2DOpt} berechnet zuerst alle wichtigen Werte, die für die size-$k$ repräsentative Skyline mit $m$ Skylinedatensätzen benötigt wird. Zeile $2$ dient dazu, dass sowohl $opt(i,t)$ als auch $optEr(i,t)$ terminieren. Das Paper \cite{Tao:2009:DRS:1546683.1547325} dieses Algorithmus berechnet in Zeile $7$ die Funktionen $optEr(k,m)$ und $opt(k,m)$ und nicht $optEr(k,m)$ und $opt(k,m)$. Dies hätte zur Folge, dass für die Berechnung der Formeln $i$ kleiner als $t$ ist und es somit nie zu einem Ergebnis kommen würde.
Das Ergebnis von $opt(m,k)$ kann als repräsentative Skyline ausgegeben werden und dem Kunden präsentiert werden.

Der Algorithmus basiert nur auf Diversität und versucht Datensätze, die möglich weit voneinander liegen zu finden, um damit alle Cluster der Skyline repräsentieren zu können. Der Algorithmus im nächsten Abschnitts versucht diesen Ansatz zu erweitern und berücksichtigt zusätzlich die Signifikanz von Datensätzen, falls diese gegeben sind.  
%% ==============================
\subsection{E-Greedy}
\label{ch:Analyse:sec:repSkyAlgos:subsec:eGreedy}
%% ==============================
Der E-Greedy Algorithmus \cite{magnani2014taking} versucht nicht nur die Diversität von Datensätzen zu maximieren, sondern auch gleichzeitig die Signifikanz zu beachten.
Signifikanz wird dadurch bestimmt, indem der User für jede Dimension seiner Wahl einen Thresholdwert oder ein Thresholdintervall angibt. Die Datensätze, die entweder den Thresholdwert übersteigen bzw. unter diesem liegen oder in dem entsprechenden Intervall liegen, werden als signifikant gesehen. Bei fehlenden Thresholds einer Dimension werden alle Datensätze bei alleiniger Betrachtung dieser Dimension als gleich signifikant gesehen. Dieser Ansatz kann noch erweitert werden, indem Thresholds durch vergangene Daten bestimmt werden. 

$$\text{Representative Skyline} = \argmax{S \in{P_k(sky(R))}} obj(S)$$

$obj(S)$ ist hier eine Funktion, die die Werte der Diversität und Signifikanz darstellt. $P_k(sky(R))$ ist eine Teilmenge mit $k$ Datensätzen der Skyline. Das Ziel ist es eine Teilmenge mit $k$ Datensätzen zu finden, die diese Objektfunktion $obj(S)$ maximiert.

$$\lambda \sum\limits_{r \in{S \backslash \{r\}}} min_{s \in S \backslash \{r\}}\delta(r,s)+(1- \lambda) \sum\limits_{r \in{S}}E(\sigma(r))$$

Die Funktion $obj(S)$ besteht aus zwei Teilen. Der Signifikanz $\sigma$ und der Diversität $\delta$.
Mit einem Gewichtungsfaktor $\lambda$ kann der User bestimmen, welcher der beiden Aspekte ihm wichtiger ist. 
Falls der User keine Thresholds angibt und dadurch alle Datensätze für jede Dimension signifikant gleich sind, wird die repräsentative Skyline nur aus diversen Datensätzen bestehen. Dieses Ergebnis kann der User betrachten und daraufhin Thresholds eingeben, um signifikante Datensätze festzulegen, falls eine reine repräsentative Skyline nicht seinen Wünschen entspricht.  Anzumerken ist, dass der \textbf{erwartete} Wert der Signifikanz $E(\sigma(r))$ berechnet wird. 

$$\begin{displaystyle}
  I_i(r, s) = \left.
  \begin{cases}
    0 & \text{if } r=s \\
    \frac{|\{o \in{sky(R)}| (r.i \geq o.i \geq s.i) \lor (s.i \geq o.i \geq r.i)\}|-1}{|sky(R)|-1} & \text{if } r \neq s
  \end{cases}
  \right.
\end{displaystyle}$$

Um die Diversität zu berechnen, wird für jede Datensatzkombination und jede Dimension $i$ die Anzahl von Skylinedatensätzen, die zwischen zwei Datensätzen liegt, gezählt. Da $I_i$ dem relativen Anteil der dazwischen liegenden Punkte entsprechen soll, wird durch die Anzahl aller Skylinedatensätze-1 geteilt.

$$\delta_{sed}(r,s)=\frac{\sum\limits\mathop{}_{\mkern-5mu i \in{[1,d]}} I_i(r,s)}{d}$$

Durch das Summieren aller $I_i$ geteilt durch die Anzahl der Dimensionen, erhält man schlussendlich die Diversität bzw. den Wert wie sehr sich die Datensätze $r$ und $s$ unterscheiden. Diesen Wert zu maximieren führt dazu, dass die Anzahl der Datensätze zwischen zwei Datensätzen maximal sein sollte. Dadurch enthält die repräsentative Skyline bei alleiniger Betrachtung von Diversität nur Datensätze, die weit voneinander entfernt sind. 

$$\sigma_{sss}(r)=\frac{logit(r)}{max\mathop{}_{\mkern-5mu s \in{sky(R)}}logit(s)}$$

Da die Thresholds durch einen User bestimmt werden, wird die für die Berücksichtigung der Signifikanz die Sigmoid Funktion benutzt, da diese User Präferenzen repräsentieren kann. Die Skyline Sigmoid Signifikanz $\sigma_{sss}$ ergibt sich durch die logistische Sigmoid Funktion des betrachteten Datensatzes geteilt durch die logistische Sigmoid Funktion des Datensatzes, der von allen Datensätzen den höchsten Wert für die logistische Sigmoid Funktion besitzt. Die logistische Sigmoid Funktion wird je nachdem, ob der Benutzer einen einzelnen Thresholdwert oder ein Thresholdintervall eingegeben hat, anders berechnet.

Wie oben genannt wird eine Eingabe eines Thresholds für die Berechnung der Signifikanz benötigt. Bei der Eingabe eines einzelnen Wertes, hier $t$ genannt, wird die logistische Sigmoidfunktion wie folgt bestimmt:

$$logit(r)=\frac{\sum\mathop{}_{\mkern-5mu i \in{[1,d]}\frac{1}{1+e^{-r.i+t.i}}}}{d}$$

Zur Berechnung der logistischen Sigmoidfunktion  wird $\frac{1}{1+e^{-r.i+t.i}}$ für alle Dimension summiert, wobei $r.i$ der Wert von Dimension $i$ des entsprechenden Datensatzes $r$ entspricht. $t.i$ entspricht dagegen dem Wert des Thresholds für diese Dimension. Diese Funktion wird benutzt falls der Threshold als untere Grenze benutzt wird und alle Datensätze mit Werten über diesem Threshold als signifikant gesehen werden.
Falls jedoch der Threshold als obere Grenze genommen wird, ändert sich die Funktion wie folgt:

$$logit(r)=\frac{\sum\mathop{}_{\mkern-5mu i \in{[1,d]}\frac{1}{1+e^{r.i-t.i}}}}{d}$$

Für Thresholdintervalls sind diejenigen Datensätze signifikant, dessen Wert in diesem Intervall liegt. Die dazugehörige Logit Funktion wird wie folgt berechnet:
$$logit(r)=\frac{\sum\limits\mathop{}_{\mkern-5mu i \in{[1,d]}}\frac{1}{b.i-a.i}(-ln(1+e^{(r-i-b.i)})+ln(1+e^{(r.i-a.i)}))}{d}$$

$a.i$ entspricht hier der unteren Grenze und $b.i$ der obere Grenze des Thresholdintervalls. 


Nach der Berechnung der Signifikanz und der Diversität, kann die Objektfunktion bestimmt werden, die hier $\bar{\delta}$ gennant wird.
$$\bar{\delta}(r,s)= \lambda \text{ } \delta(r,s)+(1- \lambda)E(\sigma(r))$$

Um nun die $k$ repräsentativen Skylinedatensätzen zu bestimmen, die diese Objektfunktion maximieren, wird der Algorithmus \ref{algo:eGreedy} benutzt. 


\begin{algorithm}[H]
\caption{E-Greedy}\label{algo:eGreedy}
\begin{algorithmic}[1]
\INPUTBF $S$ skyline, $k$ int
\State RS $:=$ emptylist
\State RS $\leftarrow$ most significant record in $S$
\State \textbf{for} $i: 1\ldots k-1$ \textbf{do}
\State \hspace{\algorithmicindent}r = $arg$ $max_{r\in{S}}$ $\bar{\delta}(r,RS)$
\State \hspace{\algorithmicindent} RS $\leftarrow$ r
\State \textbf{end for}
\State \textbf{return} RS
\end{algorithmic}
\end{algorithm}

Der erste Datensatz der zur repräsentativen Skyline hinzugefügt wird, ist der Datensatz mit der höchsten Signifikanz. Falls mehrere die gleiche Signifikanz besitzen, wird ein Datensatz von diesen zufällig ausgewählt.
Anschließend werden $k-1$ Datensätze hinzugefügt, die mit der bereits berechneten repräsentativen Skyline die Objektfunktion maximieren.

Der Vorteil des E-Greedy liegt darin, dass er sowohl Signifikanz als auch Diversität betrachtet. Ansätze zur Berechnung von repräsentativen Skylines beruhen meistens nur auf einer der beiden Kriterien.  
\enquote{Existing approaches have only focused on partial aspects of this problem. Some try to identify sets of diverse records giving an overall approximation of the skyline.}
\enquote{Others exploit some knowledge of the record scoring function to identify the most significant record, but not sets of records representative of the whole skyline.} \cite[p. 1]{magnani2014taking}
 
Die Signifikanz den User bestimmen zu lassen, liefert eine repräsentative Skyline für seine Präferenzen entsprechend. Falls nun noch die Diversität betrachtet wird, können auch Ausreißer und die Verschiedenheit von Datensätzen berücksichtigt werden. Dies ermöglicht auch repräsentative Skylines zu berechnen für die der User nur bei bestimmten Dimensionen Thresholds festlegen kann.
Der Nachteil dieses Algorithmus ist, dass der Gewichtungsfaktor $\lambda$ die repräsentative Skyline stark verändert und der User oft nicht weiß wie er die Diversität und die Signifikanz gegeneinander gewichten soll. Dies bedeutet, dass es dazu kommen kann, dass die repräsentative Skyline mehr als nur einmal berechnet wird, um ein zufrieden stellendes Ergebnis zu finden.
%% ==============================
\section{KNIME}
\label{ch:Analyse:sec:knime}
%% ==============================
Wie schon in Abschnitt \ref{ch:Grundlagen:sec:knime} erwähnt, wird für die Implementierung KNIME benutzt. Ein Node in KNIME besteht aus mehreren wichtigen Klassen, die in diesem Kapitel vorgestellt werden. Die Klasse NodeFactory erstellt von all diesen Klassen eine Instanz und wird vom Eclipse Wizard immer automatisch erstellt.
%% ==============================
\subsection{NodeModel}
\label{ch:Analyse:sec:knime:subsec:nodeModel}
%% ==============================
Das NodeModel ist dafür zuständig den Algorithmus auszuführen. Dafür müssen drei Methoden überschrieben werden: $configure$, $execute$ und $reset$.

Die $configure$ Methode soll so implementiert werden, dass sie für den Node überprüft, ob die Inputs des Nodes die korrekten Datentypen besitzen und nicht leer sind. 
Inputs können zum Beispiel Datenbankverbindungen (DatabasePortObject) oder Datentabellen (BufferedDataTable) sein. Alle diese Inputs implementieren die Schnittstelle PortObject und können in der KNIME API nachgeschlagen werden.\footnote{https://tech.knime.org/docs/api/org/knime/core/node/port/PortObject.html}
Jedoch werden der $configure$ Methode nur die wichtigsten Informationen mithilfe eines Spec Objekts(z.B. BufferedDataTableSpec) übergeben und so kann hier nicht getestet werden, ob es bei Datentabellen einzelne fehlende Werte gibt.
Falls die Eingaben inkorrekt sind, kann eine Exception geworfen werden, die dem User einen Fehler anzeigt.(siehe \ref{img:nodeModelError})

\begin{figure}[H]
	\centering
	\includegraphics[width=1.1\textwidth]{nodeModelError.png}
	\caption{Fehleranzeige eines Nodes}
	\label{img:nodeModelError}
\end{figure}

Die Inputs können auch darauf getestet werden, ob sie mit dem Algorithmus des Nodes kompatibel sind. Falls ein Algorithmus nur mit numerischen Werten arbeiten kann, kann in der Methode überprüft werden, ob einzelne Datenspalten eines BufferedDataTables nur numerische Daten enthalten und ansonsten wird ein Fehler an den User ausgegeben.
Weiterhin werden in der $configure$ Methode PortObjectSpecs zurückgegeben, die Informationen über den Output haben. Falls der Output, der in der $execute$ Methode entsteht nicht mit diesen Informationen übereinstimmt, gibt es eine Fehlermeldung und der Node kann nicht vollständig ausgeführt werden.

Die nächste Methode $execute$ bekommt alle Inputs und nicht nur deren Metainformationen als Eingabeparameter und gibt den Output aus. In dieser Methode wird der zu implementierende Algorithmus ausgeführt und ist somit die rechenintensivste Methode des ganzen Nodes. 

Falls der Node vom User zurückgesetzt werden soll oder einer der vorherigen Nodes im Datenfluss in einen ungültigen Zustand gelangt, können alle Variablen in der $reset$ Methode zurückgesetzt werden, so dass keine ungültigen Views oder Einstellungen im NodeDialog oder NodeModel vorhanden sind. 

NodeModels können SettingsModels besitzen, die die Eingabe des Users im NodeDialog speichern und für den Algorithmus verwenden. Die Methoden $loadValidatedSettingsFrom$, $saveSettingsTo$, $loadInternals$ und $saveInternals$ dienen dazu bestimmte Variablen und die genannten SettingsModels zu speichern und diese beim wieder öffnen von KNIME zu laden. Eine weitere Funktion dieser Methoden wird im nächsten Abschnitt erläutert.
%% ==============================
\subsection{NodeDialog}
\label{ch:Analyse:sec:knime:subsec:nodeDialog}
%% ==============================
Diese Klasse dient dazu dem User eine Möglichkeit zu bieten, bestimmte Parameter für den Algorithmus selbst einstellen zu können. 
Dazu können entweder Standardkomponenten von KNIME verwendet werden oder es wird eine eigene GUI dem NodeDialog hinzugefügt.

Um diese Einstellungen der NodeModel Klasse und somit dem Algorithmus zu übergeben, werden die vorher erwähnten NodeSettings oder SettingModels benutzt. Mit den load und save Methoden des NodeDialogs können diese Settings an das NodeModel übergeben werden. Im NodeModel können diese dann mit der $loadValidatedSettingsFrom$ Methode geladen werden. Diese Methoden erhalten nur ein einziges NodeSettings Objekt, welches alle anderen SettingsModels oder NodeSettings enthält. Diese Objekte können mit einem bestimmten Key dem NodeSettings Objekt hinzugefügt oder geladen werden.

In Abbildung \ref{img:nodeDialog} ist ein Beispiel eines NodeDialogs zu sehen, für den eine Datenbank-URL, ein JDBC Treiber und weitere Eingaben eingegeben werden kann.

\begin{figure}[H]
	\centering
	\includegraphics{nodeDialog.pdf}
	\caption{NodeDialogs des HAMMELGAMMELSCHAMMEL}
	\label{img:nodeDialog}
\end{figure}

%% ==============================
\subsection{NodeView}
\label{ch:Analyse:sec:knime:subsec:nodeView}
%% ==============================
Die letzte Klasse, die von großer Relevanz ist, erzeugt eine Darstellung des Outputs. Dies kann je nach Input/Output variieren. (Histogramme, Koordinatensysteme mit Datenpunkten, Pie Charts, etc.)
Da jeder View eines Nodes nicht gespeichert wird, muss er bei jedem Öffnen von KNIME neu erzeugt werden.
Deswegen gibt es die Methoden $loadInternals$ und $saveInternals$ im NodeModel, die die Daten, die zur Erzeugung des Views benötigt werden in einer externen Datei speichern. Wie die SettingModels wird die Datei mit einem speziellen Key gespeichert und geladen. 

In Abbildung \ref{img:nodeView} ist der View des fsfrsfs zu sehen, der als Input zwei BufferedDataTables bekommt und die beinhaltenden Punkte in einem Koordinatensystem anzeigt. 

\begin{figure}
	\centering
	\includegraphics[scale=0.8]{nodeView.pdf}
	\caption{View für den SkylineVisualizer Node}
	\label{img:nodeView}
\end{figure}

Nähere Informationen zur Funktionsweise und Implementierung von Nodes können auf der Webseite \footnote{https://tech.knime.org/developers} und dem Paper \cite{BCDG+07} von KNIME gefunden werden.
%% ==============================
%%% End: 
